<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="NeuComBack benchmark and self-evolving translation for IR-to-assembly neural compilation.">

  <!-- ===== Use Bulma (your repo already has these files) ===== -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css"> <!-- 你的自定义样式；可替换为 Nerfies 的 index.css 以更像 -->

  <!-- Social meta -->
  <meta property="og:title" content="QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code">
  <meta property="og:description" content="NeuComBack benchmark and self-evolving translation for IR-to-assembly neural compilation.">
</head>
<body>

  <!-- ===== Title / Authors (centered narrow column like Nerfies) ===== -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title">
            QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code
          </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
            <a href="">Hainan Fang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
            <a href="">Yuanbo Wen</a><sup>1</sup>,
            </span>
            <span class="author-block">
            <a href="">Jun Bi</a><sup>1</sup>,
            </span>
            <span class="author-block">
            <a href="">Yihan Wang</a><sup>1, 2</sup>,
            </span>
            <span class="author-block">
            <a href="">Tonghui He</a><sup>1, 2</sup>,
            </span>
            <span class="author-block">
            <a href="">Yanlin Tang</a><sup>1, 2</sup>,
            </span>
            <span class="author-block">
            <a href="">Di Huang</a><sup>1</sup>,
            </span>
            <span class="author-block">
            <a href="">Jiaming Guo</a><sup>1</sup>,
            </span>
            <span class="author-block">
            <a href="">Rui Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
            <a href="">Qi Guo</a><sup>1</sup>,
            </span>
            <span class="author-block">
            <a href="">Yunji Chen</a><sup>1, 2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
            <sup>1</sup>State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences
            </span>
            <span class="author-block">
            <sup>2</sup>University of Chinese Academy of Sciences
            </span>
          </div>
<!-- 
          <p class="mt-2">
            <span class="tag is-dark is-medium">NeurIPS 2025</span>
          </p> -->

          <!-- Link buttons (Nerfies-style rounded dark/light buttons) -->
          <div class="buttons is-centered mt-4 publication-links">
            <!-- TODO: 替换为你的真实链接 -->
            <!-- NeurIPS 2025 -->
            <a class="button is-dark is-rounded" href="https://neurips.cc/Conferences/2025" target="_blank">
              <span class="icon">
                <img src="static/images/neurips-navbar-logo.svg" alt="NeurIPS 2025" style="height:1.2em; vertical-align:middle;">
              </span>
              <span>NeurIPS&nbsp;2025</span>
            </a>
            <!-- arXiv -->
            <a class="button is-dark is-rounded" href="https://arxiv.org/abs/xxxx.xxxxx" target="_blank">
              <span class="icon"><i class="fas fa-book-open"></i></span>
              <span>arXiv</span>
            </a>
            <!-- Paper -->
            <a class="button is-dark is-rounded" href="static/paper.pdf" target="_blank">
              <span class="icon"><i class="fas fa-file-pdf"></i></span>
              <span>PDF</span>
            </a>
            <!-- Code -->
            <a class="button is-dark is-rounded" href="https://github.com/FangHainannn/QiMeng-NeuComBack" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>Code</span>
            </a>
            <!-- Dataset -->
            <a class="button is-dark is-rounded" href="https://github.com/FangHainannn/QiMeng-NeuComBack" target="_blank">
              <span class="icon"><i class="fas fa-database"></i></span>
              <span>Dataset</span>
            </a>
            <!-- BibTeX -->
            <a class="button is-dark is-rounded" href="#bibtex">
              <span class="icon"><i class="fas fa-quote-right"></i></span>
              <span>BibTeX</span>
            </a>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- ===== Overview ===== -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <h2 class="title is-3 has-text-centered">Overview</h2>
          <div class="content is-size-6">
            <p>
              Compilers, while essential, are notoriously complex systems that demand prohibitively expensive human expertise to develop and maintain. The recent advancements in Large Language Models (LLMs) offer a compelling new paradigm: Neural Compilation, which could potentially simplify compiler development for new architectures and facilitate the discovery of innovative optimization techniques.
              <br><br>
              However, several critical obstacles impede its practical adoption. Firstly, a significant lack of dedicated benchmarks and robust evaluation methodologies hinders objective assessment and tracking of progress in the field. Secondly, systematically enhancing the reliability and performance of LLM-generated assembly remains a critical challenge.
              <br><br>
              Addressing these challenges, this paper introduces NeuComBack, a novel benchmark dataset specifically designed for IR-to-assembly compilation. Leveraging this dataset, we first define a foundational Neural Compilation workflow and conduct a comprehensive evaluation of the capabilities of recent frontier LLMs on Neural Compilation, establishing new performance baselines. We further propose a self-evolving prompt optimization method that enables LLMs to iteratively evolve their internal prompt strategies by extracting insights from prior self-debugging traces, thereby enhancing their neural compilation capabilities. 
              <br><br>
              Experiments demonstrate that our method significantly improves both the functional correctness and the performance of LLM-generated assembly code. Compared to baseline prompts, the functional correctness rates improved from 44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More significantly, among the 16 correctly generated x86_64 programs using our method, 14 (87.5%) surpassed clang-O3 performance. These consistent improvements across diverse architectures (x86_64 and aarch64) and program distributions (NeuComBack L1 and L2) validate our method's superiority over conventional approaches and its potential for broader adoption in low-level neural compilation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- ===== NeuComBack Dataset ===== -->
  <section id="dataset" class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
  
          <h2 class="title is-3 has-text-centered">NeuComBack Dataset</h2>
  
          <!-- 概述 -->
          <div class="content is-size-6">
            <p>
              We construct a comprehensive dataset for the LLVM IR → Assembly (ASM) compilation task.
              As no public dataset specifically targets IR-to-ASM translation, we adapt two established
              C→ASM benchmarks: <code>ExeBench</code>, a widely used collection of C programs, and the Test Suite for Vectorizing Compilers (<code>TSVC</code>), a compiler performance benchmark. The distinct characteristics of these sources naturally lead to a two-tiered dataset:
            </p>
          </div>
  
          <!-- L1/L2 两级结构 -->
          <div class="columns is-multiline">
            <div class="column is-6">
              <div class="box">
                <h3 class="title is-5 mb-2">Level 1 (Fundamental Compilation, 200 tasks)</h3>
                <p>
                  This selection from the <code>ExeBench</code> test suite covers a broad variety of real-world C programs and features diverse control-flow patterns. A significant portion of these programs, derived from embedded systems applications, exhibits intensive I/O operations. While extremely complex control flow (e.g., deep recursion, concurrency) is underrepresented in available sources, L1 is designed with a breadth-first focus to assess general-purpose functional correctness.
                </p>
              </div>
            </div>
            <div class="column is-6">
              <div class="box">
                <h3 class="title is-5 mb-2">Level 2 (Optimization Potential, 151 tasks)</h3>
                <p>
                  This level, drawn from the <code>TSVC</code> benchmark suite, features programs characterized by computationally simple execution paths but notable loop intricacy. Such programs provide a strong basis for assessing the optimization capabilities reflected in the generated assembly.
                </p>
              </div>
            </div>
          </div>
  
        </div>
      </div>
    </div>
  </section>

  <!-- ===== Method ===== -->
  <section id="method" class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
  
          <h2 class="title is-3 has-text-centered">Method</h2>
  
          <!-- Pipeline 图 -->
          <img class="image" src="static/images/overview.png" alt="Teaser for self-evolving neural compilation" style="width:100%; height:auto;">
          <p class="has-text-grey is-size-6 mt-2 has-text-centered">
            Figure 1: Pipeline of our automatic prompt learning method on <i>Neural Compilation</i>.
          </p>
  
          <!-- Overview -->
          <div class="content is-size-6">
            <p>
              Our Neural Compilation framework leverages a novel automatic prompt-learning mechanism specifically tailored to improve LLM-generated assembly code. Distinct from previous prompt evolution methods, our approach's core insight lies in learning from the LLM's complete iterative self-debugging process, which effectively enables the LLM to learn from its past practices in diagnosing and resolving complex errors in assembly code generation. Our method comprises two distinct stages: offline prompt learning and online inference. In the offline stage, the model iteratively evolves prompts based on comprehensive analysis and insights derived from past generation trials. Subsequently, in the online stage, the model utilizes these refined prompts to generate and iteratively optimize assembly code, progressively improving quality and performance.
            </p>
          </div>
  
          <!-- Offline: Prompt Learning -->
          <div class="box">
            <h4 class="title is-5 mb-2">Offline: Prompt Learning</h4>
            <p class="content is-size-6 mb-2">
              The offline learning phase focuses on automatically discovering and evolving prompts to effectively guide the LLM toward generating correct and performant assembly code.
            </p>
            <ol class="content is-size-6 ml-5">
              <li>This process begins with <b>initializing</b> an empty prompt template.</li>
              <li>Following this, <b>self-debug trials</b> are collected by using the LLM to perform a ``compilation'' process and then test the generated assembly translations; errors trigger iterative self-debugging, refining code until correctness or iteration limits are reached.</li>
              <li>Subsequently, <b>critical insights</b> are extracted by analyzing the complete self-debug trajectories to identify error patterns and effective strategies via LLM-assisted analysis.</li>
              <li>Finally, <b>prompt evolution</b> occurs by refining prompts through the integration of experience and insights (extracted from a batch of self-debug trials), which are then reviewed for clarity and effectiveness.</li>
            </ol>
          </div>
  
          <!-- Online: Inference -->
          <div class="box">
            <h4 class="title is-5 mb-2">Online: Inference</h4>
            <p class="content is-size-6 mb-2">
              The online inference stage deploys evolved prompts from offline learning to iteratively generate and optimize assembly code.
            </p>
            <ol class="content is-size-6 ml-5">
              <li>First, the <b>initial assembly generation</b> is performed, using evolved prompts to generate initial assembly code from IR. This initial assembly is then tested, and iterative self-debugging is triggered until functional correctness is achieved; if self-debug fails after all attempts, the generation is terminated, and a failure is reported.</li>
              <li>Next, <b>iterative optimization</b> is applied to further refine the initial assembly code, with evolved prompts still provided to minimize error introduction. Each optimization iteration includes testing and self-debugging as necessary to ensure continued correctness.</li>
              <li>Finally, the process <b>outputs</b> the optimized assembly code, demonstrating competitive or superior performance compared to compiler-generated code.</li>
            </ol>
          </div>

        </div>
      </div>
    </div>
  </section>

  <!-- ===================== Main Results ===================== -->
  <section id="main-results" class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
  
          <h2 class="title is-3 has-text-centered">Main Results</h2>
  
          <!-- ===== 1) Baseline: Existing LLMs on NeuComBack-L2 (x86_64) ===== -->
          <div class="box">
            <h3 class="title is-5 mb-2">Empirical Evaluation of Existing LLMs</h3>
            <p class="content is-size-6">
              We benchmark state-of-the-art LLMs on <b>NeuComBack-L2</b> (x86_64, 151 cases) to establish baselines for
              <i>Neural Compilation</i>. Reasoning-enhanced models show clear advantages: <b>DeepSeek-R1</b> achieves the strongest baseline
              with <b>45.70% ACC</b> and <b>21.85% ACC+Perf</b>, while <b>O3-Mini</b>/<b>O1</b> also perform notably better than
              non-reasoning-specialized models.
            </p>
            <div class="table-container">
              <table class="table is-striped is-hoverable is-fullwidth">
                <caption class="has-text-weight-semibold">Table 1: Baseline performance of advanced LLMs, NeuComBack-L2 (overall 151 cases), x86_64</caption>
                <thead>
                  <tr><th>Model</th><th>ACC (%)</th><th>ACC+Perf (%)</th></tr>
                </thead>
                <tbody>
                  <tr><td>GPT-4o</td><td>1.99 (3/151)</td><td>0.66 (1/151)</td></tr>
                  <tr><td>O3-Mini</td><td>21.19 (32/151)</td><td>5.30 (8/151)</td></tr>
                  <tr><td>O1</td><td>19.87 (30/151)</td><td>5.30 (8/151)</td></tr>
                  <tr><td>DeepSeek-V3</td><td>14.57 (22/151)</td><td>3.31 (5/151)</td></tr>
                  <tr><td><b>DeepSeek-R1</b></td><td><b>45.70 (69/151)</b></td><td><b>21.85 (33/151)</b></td></tr>
                </tbody>
              </table>
            </div>
            <p class="content is-size-6 has-text-grey">
              ACC: functional correctness; ACC+Perf: correctness with runtime better than <code>clang -O3</code>.
            </p>
          </div>
  
          <!-- ===== 2) Our Method on NeuComBack (x86_64) ===== -->
          <div class="box">
            <h3 class="title is-5 mb-2">Effect of Self-Evolving Prompt Optimization</h3>
            <p class="content is-size-6">
              Using <b>DeepSeek-R1</b> on <b>NeuComBack-L1</b> (x86_64), our learned prompt greatly boosts correctness:
              <b>50.00% → 80.00% ACC</b> on the test set (40 samples), a relative improvement of ~60%.
            </p>
            <div class="table-container">
              <table class="table is-striped is-hoverable is-fullwidth">
                <caption class="has-text-weight-semibold">Table 2: Performance of automatic prompt learning vs. baseline prompt on NeuComBack-L1 (test set, 40 samples), x86_64, DeepSeek-R1.</caption>
                <thead>
                  <tr><th>Method</th><th>ACC (%)</th></tr>
                </thead>
                <tbody>
                  <tr><td>Baseline</td><td>50.00 (20/40)</td></tr>
                  <tr><td><b>Our</b></td><td><b>80.00 (32/40)</b></td></tr>
                </tbody>
              </table>
            </div>
            <p class="content is-size-6">
              On <b>NeuComBack-L2</b> (x86_64, test 25), our learned prompt improves initial correctness
              from <b>44.00% → 64.00%</b> and boosts high-performance solutions from <b>24.00% → 40.00%</b>.
              After two rounds of iterative optimization, the advantage amplifies: <b>ACC+Perf 28.00% → 56.00%</b>
              (<b>2×</b> relative improvement). Notably, among 16 correct programs from our method,
              <b>14 (87.5%)</b> surpass <code>-O3</code>.
            </p>
            <div class="table-container">
              <table class="table is-striped is-hoverable is-fullwidth">
                <caption class="has-text-weight-semibold">Table 3: Performance of automatic prompt learning vs. baseline prompt on NeuComBack-L2 (test set, 25 samples), x86_64, DeepSeek-R1.</caption>
                <thead>
                  <tr><th>Method &amp; Stage</th><th>ACC (%)</th><th>ACC+Perf (%)</th></tr>
                </thead>
                <tbody>
                  <tr><td><b>Baseline Prompt</b> — After Initial Generation</td><td>44.00 (11/25)</td><td>24.00 (6/25)</td></tr>
                  <tr><td><b>Baseline Prompt</b> — After 2 Rounds Iter. Opt.</td><td>--</td><td>28.00 (7/25)</td></tr>
                  <tr><td><b>Our (Learned Prompt)</b> — After Initial Generation</td><td><b>64.00 (16/25)</b></td><td><b>40.00 (10/25)</b></td></tr>
                  <tr><td><b>Our (Learned Prompt)</b> — After 2 Rounds Iter. Opt.</td><td>--</td><td><b>56.00 (14/25)</b></td></tr>
                </tbody>
              </table>
            </div>
          </div>

          <!-- ===== 3) aarch64 ===== -->
          <div class="box">
            <h3 class="title is-5 mb-2">Generalization to aarch64</h3>
            <p class="content is-size-6">
              On <b>aarch64</b> (NeuComBack-L2, test 25), our method substantially improves both correctness and performance:
              <b>ACC 36.00% → 72.00%</b> and <b>ACC+Perf 8.00% → 28.00%</b>, showing strong applicability across different instruction sets.
            </p>
            <div class="table-container">
              <table class="table is-striped is-hoverable is-fullwidth">
                <caption class="has-text-weight-semibold">Table 4: Effectiveness of automatic prompt learning, DeepSeekR1, NeuComBack-L2 (test set, 25 samples), aarch64</caption>
                <thead>
                  <tr><th>Method</th><th>ACC (%)</th><th>ACC+Perf (%)</th></tr>
                </thead>
                <tbody>
                  <tr><td>Baseline</td><td>36.00 (9/25)</td><td>8.00 (2/25)</td></tr>
                  <tr><td><b>Our</b></td><td><b>72.00 (18/25)</b></td><td><b>28.00 (7/25)</b></td></tr>
                </tbody>
              </table>
            </div>
          </div>
  
          <!-- ===== 4) Transferability across Distributions ===== -->
          <div class="box">
            <h3 class="title is-5 mb-2">Transferability across Data Distributions</h3>
            <p class="content is-size-6">
              Prompts learned on <b>NeuComBack-L2</b> transfer positively to <b>NeuComBack-L1</b> (x86_64):
              test ACC improves from <b>50.00% → 67.50%</b> without additional learning, and overall ACC reaches
              <b>74.50%</b> across all 200 L1 cases.
            </p>
            <div class="table-container">
              <table class="table is-striped is-hoverable is-fullwidth">
                <caption class="has-text-weight-semibold">Table 5: Performance on NeuComBack-L1 (x86_64, DeepSeek-R1) using different prompt strategies, showing test set and overall ACC</caption>
                <thead>
                  <tr><th>Prompt Strategy</th><th>Test Set ACC (%)</th><th>Overall ACC (%)</th></tr>
                </thead>
                <tbody>
                  <tr><td>Default Prompt</td><td>50.00 (20/40)</td><td>54.50 (109/200)</td></tr>
                  <tr><td><b>Learned on NeuComBack-L1</b></td><td><b>80.00 (32/40)</b></td><td>−</td></tr>
                  <tr><td><b>Learned on NeuComBack-L2</b></td><td><b>67.50 (27/40)</b></td><td><b>74.50 (149/200)</b></td></tr>
                </tbody>
              </table>
            </div>
          </div>
  
          <!-- ===== 5) Fewer Self-Debug Rounds ===== -->
          <div class="box">
            <h3 class="title is-5 mb-2">Efficiency: Fewer Self-Debug Rounds</h3>
            <p class="content is-size-6">
              For correctly solved programs, our method consistently reduces the average number of self-debug rounds across
              architectures and datasets, indicating faster convergence to correct and performant assembly.
            </p>
            <div class="table-container">
              <table class="table is-striped is-hoverable is-fullwidth">
                <caption class="has-text-weight-semibold">Table 6: Average self-debug rounds for successfully compiled programs by DeepSeek-R1 on test sets, comparing our method with the baseline. Lower is better.</caption>
                <thead>
                  <tr>
                    <th>Architecture</th><th>Dataset</th><th>Max Debug Rounds</th><th>Baseline</th><th>Our Method</th>
                  </tr>
                </thead>
                <tbody>
                  <tr><td rowspan="2">x86_64</td><td>NeuComBack-L1</td><td>1</td><td>0.90</td><td><b>0.28</b></td></tr>
                  <tr>                              <td>NeuComBack-L2</td><td>2</td><td>1.09</td><td><b>0.25</b></td></tr>
                  <tr><td>aarch64</td><td>NeuComBack-L2</td><td>4</td><td>1.44</td><td><b>1.22</b></td></tr>
                </tbody>
              </table>
            </div>
          </div>
  
        </div>
      </div>
    </div>
  </section>
<!-- ===== Examples: IR → Assembly ===== -->
<section id="examples" class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
  
          <h2 class="title is-3 has-text-centered">Examples: IR → Assembly</h2>
          <!-- <p class="has-text-centered has-text-grey is-size-6 mb-5">
            Two end-to-end translations on x86_64. Click to expand IR / ASM.
          </p> -->
  
          <!-- ========== Example 1 ========== -->
          <div class="box examples-box">
            <div class="level examples-narrow">
              <div class="level-left">
                <h3 class="title is-5 mb-0">Example&nbsp;1：<code>s331</code>（x86_64）</h3>
              </div>
              <div class="level-right">
                <span class="tag is-dark is-light">IR → ASM</span>
              </div>
            </div>
  
            <div class="content is-size-6 examples-narrow">
              <details class="examples-details" open>
                <summary><strong>Input IR (LLVM)</strong></summary>
                <div class="code-scroller">
                  <pre><code class="language-llvm">; ModuleID = 'tmp/s331_inner.c'
source_filename = "tmp/s331_inner.c"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@a = external global [32000 x float], align 64
@b = external global [32000 x float], align 64
@c = external global [32000 x float], align 64
@d = external global [32000 x float], align 64
@e = external global [32000 x float], align 64
@aa = external global [256 x [256 x float]], align 64
@bb = external global [256 x [256 x float]], align 64
@cc = external global [256 x [256 x float]], align 64

; Function Attrs: nounwind uwtable
define dso_local i32 @s331_inner() local_unnamed_addr #0 {
  br label %1

1:                                                ; preds = %0, %5
  %2 = phi i32 [ 0, %0 ], [ %8, %5 ]
  br label %10

3:                                                ; preds = %5
  %4 = add nsw i32 %41, 1
  ret i32 %4

5:                                                ; preds = %10
  %6 = sitofp i32 %41 to float
  %7 = tail call i32 @dummy(ptr noundef nonnull @a, ptr noundef nonnull @b, ptr noundef nonnull @c, ptr noundef nonnull @d, ptr noundef nonnull @e, ptr noundef nonnull @aa, ptr noundef nonnull @bb, ptr noundef nonnull @cc, float noundef %6) #2
  %8 = add nuw nsw i32 %2, 1
  %9 = icmp eq i32 %8, 100000
  br i1 %9, label %3, label %1, !llvm.loop !5

10:                                               ; preds = %10, %1
  %11 = phi i64 [ 0, %1 ], [ %42, %10 ]
  %12 = phi i32 [ -1, %1 ], [ %41, %10 ]
  %13 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %11
  %14 = load float, ptr %13, align 4, !tbaa !7
  %15 = fcmp olt float %14, 0.000000e+00
  %16 = trunc i64 %11 to i32
  %17 = select i1 %15, i32 %16, i32 %12
  %18 = add nuw nsw i64 %11, 1
  %19 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %18
  %20 = load float, ptr %19, align 4, !tbaa !7
  %21 = fcmp olt float %20, 0.000000e+00
  %22 = trunc i64 %18 to i32
  %23 = select i1 %21, i32 %22, i32 %17
  %24 = add nuw nsw i64 %11, 2
  %25 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %24
  %26 = load float, ptr %25, align 4, !tbaa !7
  %27 = fcmp olt float %26, 0.000000e+00
  %28 = trunc i64 %24 to i32
  %29 = select i1 %27, i32 %28, i32 %23
  %30 = add nuw nsw i64 %11, 3
  %31 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %30
  %32 = load float, ptr %31, align 4, !tbaa !7
  %33 = fcmp olt float %32, 0.000000e+00
  %34 = trunc i64 %30 to i32
  %35 = select i1 %33, i32 %34, i32 %29
  %36 = add nuw nsw i64 %11, 4
  %37 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %36
  %38 = load float, ptr %37, align 4, !tbaa !7
  %39 = fcmp olt float %38, 0.000000e+00
  %40 = trunc i64 %36 to i32
  %41 = select i1 %39, i32 %40, i32 %35
  %42 = add nuw nsw i64 %11, 5
  %43 = icmp eq i64 %42, 32000
  br i1 %43, label %5, label %10, !llvm.loop !11
}

declare i32 @dummy(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, float noundef) local_unnamed_addr #1

attributes #0 = { nounwind uwtable "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #1 = { "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #2 = { nounwind }

!llvm.module.flags = !{!0, !1, !2, !3}
!llvm.ident = !{!4}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 8, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 7, !"uwtable", i32 2}
!4 = !{!"clang version 17.0.6"}
!5 = distinct !{!5, !6}
!6 = !{!"llvm.loop.mustprogress"}
!7 = !{!8, !8, i64 0}
!8 = !{!"float", !9, i64 0}
!9 = !{!"omnipotent char", !10, i64 0}
!10 = !{!"Simple C/C++ TBAA"}
!11 = distinct !{!11, !6}
                  </code></pre>
                </div>
              </details>
  
              <details class="examples-details">
                <summary><strong>Generated Assembly (x86_64)</strong></summary>
                <div class="code-scroller">
                  <pre><code class="language-nasm">.text
.globl s331_inner
.type s331_inner, @function
s331_inner:
    pushq %rbp
    movq %rsp, %rbp
    pushq %r12
    pushq %r13
    pushq %r14
    leaq a(%rip), %r14
    xorl %r12d, %r12d
    pxor %xmm1, %xmm1

outer_loop:
    xorl %eax, %eax
    movl $-1, %r13d

.align 16
vector_loop:
    movaps (%r14, %rax, 4), %xmm0
    cmpltps %xmm1, %xmm0
    movmskps %xmm0, %ecx
    testl %ecx, %ecx
    jz .Lno_update0
    bsrl %ecx, %ecx
    leal (%rax, %rcx), %edx
    cmpl %r13d, %edx
    cmovgl %edx, %r13d
.Lno_update0:

    movaps 16(%r14, %rax, 4), %xmm2
    cmpltps %xmm1, %xmm2
    movmskps %xmm2, %ecx
    testl %ecx, %ecx
    jz .Lno_update1
    bsrl %ecx, %ecx
    leal 4(%rax, %rcx), %edx
    cmpl %r13d, %edx
    cmovgl %edx, %r13d
.Lno_update1:

    addq $8, %rax
    cmpq $32000, %rax
    jl vector_loop

    cvtsi2ssl %r13d, %xmm0
    subq $32, %rsp
    leaq a(%rip), %rdi
    leaq b(%rip), %rsi
    leaq c(%rip), %rdx
    leaq d(%rip), %rcx
    leaq e(%rip), %r8
    leaq aa(%rip), %r9
    leaq bb(%rip), %rax
    movq %rax, (%rsp)
    leaq cc(%rip), %rax
    movq %rax, 8(%rsp)
    movss %xmm0, 16(%rsp)
    call dummy@PLT
    addq $32, %rsp

    incl %r12d
    cmpl $100000, %r12d
    jne outer_loop

    movl %r13d, %eax
    addl $1, %eax
    popq %r14
    popq %r13
    popq %r12
    popq %rbp
    ret

.size s331_inner, .-s331_inner

.section .bss
.comm a, 128000, 64
.comm b, 128000, 64
.comm c, 128000, 64
.comm d, 128000, 64
.comm e, 128000, 64
.comm aa, 262144, 64
.comm bb, 262144, 64
.comm cc, 262144, 64
                  </code></pre>
                </div>
              </details>
            </div>
          </div>
  
          <!-- ========== Example 2 ========== -->
          <div class="box examples-box">
            <div class="level examples-narrow">
              <div class="level-left">
                <h3 class="title is-5 mb-0">Example&nbsp;2：<code>s332</code>（x86_64）</h3>
              </div>
              <div class="level-right">
                <span class="tag is-dark is-light">IR → ASM</span>
              </div>
            </div>
  
            <div class="content is-size-6 examples-narrow">
              <details class="examples-details" open>
                <summary><strong>Input IR (LLVM)</strong></summary>
                <div class="code-scroller">
                  <pre><code class="language-llvm">; ModuleID = 'tmp/s332_inner.c'
source_filename = "tmp/s332_inner.c"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

@a = external global [32000 x float], align 64
@b = external global [32000 x float], align 64
@c = external global [32000 x float], align 64
@d = external global [32000 x float], align 64
@e = external global [32000 x float], align 64
@aa = external global [256 x [256 x float]], align 64
@bb = external global [256 x [256 x float]], align 64
@cc = external global [256 x [256 x float]], align 64

; Function Attrs: nounwind uwtable
define dso_local float @s332_inner(i32 noundef %0) local_unnamed_addr #0 {
  %2 = sitofp i32 %0 to float
  br label %3

3:                                                ; preds = %1, %37
  %4 = phi i32 [ 0, %1 ], [ %43, %37 ]
  br label %5

5:                                                ; preds = %30, %3
  %6 = phi i64 [ 0, %3 ], [ %31, %30 ]
  %7 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %6
  %8 = load float, ptr %7, align 4, !tbaa !5
  %9 = fcmp ogt float %8, %2
  br i1 %9, label %33, label %10

10:                                               ; preds = %5
  %11 = add nuw nsw i64 %6, 1
  %12 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %11
  %13 = load float, ptr %12, align 4, !tbaa !5
  %14 = fcmp ogt float %13, %2
  br i1 %14, label %33, label %15

15:                                               ; preds = %10
  %16 = add nuw nsw i64 %6, 2
  %17 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %16
  %18 = load float, ptr %17, align 4, !tbaa !5
  %19 = fcmp ogt float %18, %2
  br i1 %19, label %33, label %20

20:                                               ; preds = %15
  %21 = add nuw nsw i64 %6, 3
  %22 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %21
  %23 = load float, ptr %22, align 4, !tbaa !5
  %24 = fcmp ogt float %23, %2
  br i1 %24, label %33, label %25

25:                                               ; preds = %20
  %26 = add nuw nsw i64 %6, 4
  %27 = getelementptr inbounds [32000 x float], ptr @a, i64 0, i64 %26
  %28 = load float, ptr %27, align 4, !tbaa !5
  %29 = fcmp ogt float %28, %2
  br i1 %29, label %33, label %30

30:                                               ; preds = %25
  %31 = add nuw nsw i64 %6, 5
  %32 = icmp eq i64 %31, 32000
  br i1 %32, label %37, label %5, !llvm.loop !9

33:                                               ; preds = %25, %20, %15, %10, %5
  %34 = phi i64 [ %6, %5 ], [ %11, %10 ], [ %16, %15 ], [ %21, %20 ], [ %26, %25 ]
  %35 = phi float [ %8, %5 ], [ %13, %10 ], [ %18, %15 ], [ %23, %20 ], [ %28, %25 ]
  %36 = trunc i64 %34 to i32
  br label %37

37:                                               ; preds = %30, %33
  %38 = phi float [ %35, %33 ], [ -1.000000e+00, %30 ]
  %39 = phi i32 [ %36, %33 ], [ -2, %30 ]
  %40 = sitofp i32 %39 to float
  %41 = fadd float %38, %40
  %42 = tail call i32 @dummy(ptr noundef nonnull @a, ptr noundef nonnull @b, ptr noundef nonnull @c, ptr noundef nonnull @d, ptr noundef nonnull @e, ptr noundef nonnull @aa, ptr noundef nonnull @bb, ptr noundef nonnull @cc, float noundef %41) #2
  %43 = add nuw nsw i32 %4, 1
  %44 = icmp eq i32 %43, 100000
  br i1 %44, label %45, label %3, !llvm.loop !11

45:                                               ; preds = %37
  ret float %38
}

declare i32 @dummy(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, float noundef) local_unnamed_addr #1

attributes #0 = { nounwind uwtable "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #1 = { "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cmov,+cx8,+fxsr,+mmx,+sse,+sse2,+x87" "tune-cpu"="generic" }
attributes #2 = { nounwind }

!llvm.module.flags = !{!0, !1, !2, !3}
!llvm.ident = !{!4}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 8, !"PIC Level", i32 2}
!2 = !{i32 7, !"PIE Level", i32 2}
!3 = !{i32 7, !"uwtable", i32 2}
!4 = !{!"clang version 17.0.6"}
!5 = !{!6, !6, i64 0}
!6 = !{!"float", !7, i64 0}
!7 = !{!"omnipotent char", !8, i64 0}
!8 = !{!"Simple C/C++ TBAA"}
!9 = distinct !{!9, !10}
!10 = !{!"llvm.loop.mustprogress"}
!11 = distinct !{!11, !10}
                  </code></pre>
                </div>
              </details>
  
              <details class="examples-details">
                <summary><strong>Generated Assembly (x86_64)</strong></summary>
                <div class="code-scroller">
                  <pre><code class="language-nasm">.section .text
.globl s332_inner
.type s332_inner, @function
s332_inner:
    pushq %rbp
    movq %rsp, %rbp
    pushq %r12
    pushq %r13
    pushq %r14
    pushq %rbx
    subq $16, %rsp
    movaps %xmm6, -16(%rbp)
    movaps %xmm7, -32(%rbp)
    leaq a(%rip), %r14
    cvtsi2ssl %edi, %xmm6
    xorl %r12d, %r12d

.Louter_loop:
    cmpl $100000, %r12d
    jge .Lend_outer_loop
    xorq %r13, %r13
    movq $-2, %rbx
    movss .LC0(%rip), %xmm7

.Linner_loop:
    cmpq $32000, %r13
    jge .Lexit_inner_loop
    mov %r13d, %edx
    leaq (%r14,%rdx,4), %rcx
    movups (%rcx), %xmm0
    movaps %xmm0, %xmm1
    cmpps $0x1E, %xmm6, %xmm1
    movmskps %xmm1, %eax
    testl %eax, %eax
    jnz .Lfound_simd
    movss 16(%rcx), %xmm0
    comiss %xmm6, %xmm0
    ja .Lfound_4
    addq $5, %r13
    jmp .Linner_loop

.Lfound_simd:
    bsfl %eax, %eax
    movss (%rcx,%rax,4), %xmm7
    leaq (%r13,%rax), %rbx
    jmp .Lexit_inner_loop

.Lfound_4:
    leaq 4(%r13), %rbx
    movss 16(%rcx), %xmm7

.Lexit_inner_loop:
    cvtsi2ssl %ebx, %xmm0
    addss %xmm7, %xmm0
    leaq a(%rip), %rdi
    leaq b(%rip), %rsi
    leaq c(%rip), %rdx
    leaq d(%rip), %rcx
    leaq e(%rip), %r8
    leaq aa(%rip), %r9
    leaq bb(%rip), %r10
    leaq cc(%rip), %r11
    subq $24, %rsp
    movq %r10, (%rsp)
    movq %r11, 8(%rsp)
    movss %xmm0, 16(%rsp)
    call dummy@PLT
    addq $24, %rsp
    incl %r12d
    jmp .Louter_loop

.Lend_outer_loop:
    movaps %xmm7, %xmm0
    movaps -32(%rbp), %xmm7
    movaps -16(%rbp), %xmm6
    addq $16, %rsp
    popq %rbx
    popq %r14
    popq %r13
    popq %r12
    popq %rbp
    ret
.size s332_inner, .-s332_inner

.section .rodata
.align 4
.LC0:
    .long 0xbf800000
                    
                  </code></pre>
                </div>
              </details>
            </div>
          </div>
  
        </div>
      </div>
    </div>
  </section>
  

  <!-- ===== BibTeX ===== -->
  <section id="bibtex" class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <h2 class="title is-3 has-text-centered">BibTeX</h2>
<pre>
@inproceedings{fang2025neucomback,
  title     = {QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code},
  author    = {Hainan Fang, Yuanbo Wen, Jun Bi, Yihan Wang, Tonghui He, Yanlin Tang, Di Huang, Jiaming Guo, Rui Zhang, Qi Guo, Yunji Chen},
  booktitle = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems},
  year      = {2025},
  note      = {NeurIPS 2025}
}
</pre>
        </div>
      </div>
    </div>
  </section>

<footer class="footer has-text-centered has-text-grey-dark">
  <div class="content is-small">
    <p>
      This website is licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
    <p>
      Thanks for the website template
      <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a>.
    </p>
  </div>
</footer>


  <!-- ===== Scripts (你仓库已有这些 JS) ===== -->
  <script src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <!-- ⚠️ 如果模板自带 analytics，请删除或换成你自己的 -->
</body>
</html>
